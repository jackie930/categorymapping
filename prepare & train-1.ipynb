{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34175b4d-e866-4699-8482-d1aa69282736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## step 0: 训练数据准备, 准备用于训练的category下的sku数据列表, 结果保存为两个csv文件\n",
    "#!python prepare_v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8eb2a6-3f2a-4c5e-aed4-811e4467285a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< 总类目数量:  517\n",
      "<< 总类目匹配数量:  100\n",
      "<< 训练匹配数量:  100\n",
      "<< 评估类目:  417\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 476.10it/s]\n",
      "100%|████████████████████████████████████████| 417/417 [00:00<00:00, 500.64it/s]\n",
      "Data written to json\n",
      "Data written to json\n",
      "train samples:498,test samples:56\n",
      "Data written to json\n"
     ]
    }
   ],
   "source": [
    "!python prepare.py --split_idx 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33290017-948d-4bd1-bc9b-17968a436a20",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00f2617-f096-42ec-9e06-e978f4c85131",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.1.5)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (2.2.0)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.4/436.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.7/782.7 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, accelerate, transformers\n",
      "Successfully installed accelerate-0.34.2 huggingface-hub-0.25.1 regex-2024.9.11 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl accelerate transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93af3d12-bb9c-423c-83eb-e752edbaf654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `8`\n",
      "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
      "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "是否有GPU可用: True\n",
      "GPU数量: 8\n",
      "Namespace(model_name='openai/clip-vit-large-patch14-336', in_channels=1032, train_data_file='../data/train/train_vit_pairwise.json', test_data_file='../data/train/test_vit_pairwise.json', image_dir='../data/images', batch_size=2, num_cls=3, max_epoch=1, lr=1e-05, weight_decay=0.005, num_workers=0, print_interval_steps=100, use_score_model=1, use_dropout=0, keep_prob=0.8, clip_value=1.0, save_path='vit_pairwise_model_emb', save_name=None)\n",
      "是否有GPU可用: True\n",
      "GPU数量: 8\n",
      "Namespace(model_name='openai/clip-vit-large-patch14-336', in_channels=1032, train_data_file='../data/train/train_vit_pairwise.json', test_data_file='../data/train/test_vit_pairwise.json', image_dir='../data/images', batch_size=2, num_cls=3, max_epoch=1, lr=1e-05, weight_decay=0.005, num_workers=0, print_interval_steps=100, use_score_model=1, use_dropout=0, keep_prob=0.8, clip_value=1.0, save_path='vit_pairwise_model_emb', save_name=None)\n",
      "是否有GPU可用: True\n",
      "GPU数量: 8\n",
      "Namespace(model_name='openai/clip-vit-large-patch14-336', in_channels=1032, train_data_file='../data/train/train_vit_pairwise.json', test_data_file='../data/train/test_vit_pairwise.json', image_dir='../data/images', batch_size=2, num_cls=3, max_epoch=1, lr=1e-05, weight_decay=0.005, num_workers=0, print_interval_steps=100, use_score_model=1, use_dropout=0, keep_prob=0.8, clip_value=1.0, save_path='vit_pairwise_model_emb', save_name=None)\n",
      "是否有GPU可用: True\n",
      "GPU数量: 8\n",
      "Namespace(model_name='openai/clip-vit-large-patch14-336', in_channels=1032, train_data_file='../data/train/train_vit_pairwise.json', test_data_file='../data/train/test_vit_pairwise.json', image_dir='../data/images', batch_size=2, num_cls=3, max_epoch=1, lr=1e-05, weight_decay=0.005, num_workers=0, print_interval_steps=100, use_score_model=1, use_dropout=0, keep_prob=0.8, clip_value=1.0, save_path='vit_pairwise_model_emb', save_name=None)\n",
      "是否有GPU可用: True\n",
      "GPU数量: 8\n",
      "Namespace(model_name='openai/clip-vit-large-patch14-336', in_channels=1032, train_data_file='../data/train/train_vit_pairwise.json', test_data_file='../data/train/test_vit_pairwise.json', image_dir='../data/images', batch_size=2, num_cls=3, max_epoch=1, lr=1e-05, weight_decay=0.005, num_workers=0, print_interval_steps=100, use_score_model=1, use_dropout=0, keep_prob=0.8, clip_value=1.0, save_path='vit_pairwise_model_emb', save_name=None)\n",
      "是否有GPU可用: True\n",
      "GPU数量: 8\n",
      "是否有GPU可用: True\n",
      "GPU数量: 8\n",
      "Namespace(model_name='openai/clip-vit-large-patch14-336', in_channels=1032, train_data_file='../data/train/train_vit_pairwise.json', test_data_file='../data/train/test_vit_pairwise.json', image_dir='../data/images', batch_size=2, num_cls=3, max_epoch=1, lr=1e-05, weight_decay=0.005, num_workers=0, print_interval_steps=100, use_score_model=1, use_dropout=0, keep_prob=0.8, clip_value=1.0, save_path='vit_pairwise_model_emb', save_name=None)\n",
      "Namespace(model_name='openai/clip-vit-large-patch14-336', in_channels=1032, train_data_file='../data/train/train_vit_pairwise.json', test_data_file='../data/train/test_vit_pairwise.json', image_dir='../data/images', batch_size=2, num_cls=3, max_epoch=1, lr=1e-05, weight_decay=0.005, num_workers=0, print_interval_steps=100, use_score_model=1, use_dropout=0, keep_prob=0.8, clip_value=1.0, save_path='vit_pairwise_model_emb', save_name=None)\n",
      "是否有GPU可用: True\n",
      "GPU数量: 8\n",
      "Namespace(model_name='openai/clip-vit-large-patch14-336', in_channels=1032, train_data_file='../data/train/train_vit_pairwise.json', test_data_file='../data/train/test_vit_pairwise.json', image_dir='../data/images', batch_size=2, num_cls=3, max_epoch=1, lr=1e-05, weight_decay=0.005, num_workers=0, print_interval_steps=100, use_score_model=1, use_dropout=0, keep_prob=0.8, clip_value=1.0, save_path='vit_pairwise_model_emb', save_name=None)\n",
      "-----!! 249 56\n",
      "-----!! 249 56\n",
      "-----!! 249 56\n",
      "-----!! 249 56\n",
      "-----!! 249 56\n",
      "-----!! 249 56\n",
      "-----!! 249 56\n",
      "-----!! 249 56\n",
      "[rank7]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank4]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "!!!!!! 31 7\n",
      "[rank1]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank5]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "  0%|                                                    | 0/31 [00:00<?, ?it/s][rank6]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "!!!!!! 31 7\n",
      "[rank3]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank2]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "  0%|                                                    | 0/31 [00:00<?, ?it/s]!!!!!! 31 7\n",
      "!!!!!! 31 7\n",
      "!!!!!! 31 7\n",
      "!!!!!! 31 7\n",
      "!!!!!! 31 7\n",
      "  0%|                                                    | 0/31 [00:00<?, ?it/s] 31 7!\n",
      "100%|███████████████████████████████████████████| 31/31 [04:03<00:00,  7.84s/it]\n",
      "Epochs: 1 | Loss:  0.377 \n",
      "Training is finished!\n",
      "100%|███████████████████████████████████████████| 31/31 [04:03<00:00,  7.84s/it]\n",
      "Epochs: 1 | Loss:  0.281 \n",
      "Training is finished!\n",
      "100%|███████████████████████████████████████████| 31/31 [04:03<00:00,  7.86s/it]\n",
      "Epochs: 1 | Loss:  0.240 \n",
      "Training is finished!\n",
      "100%|███████████████████████████████████████████| 31/31 [04:03<00:00,  7.87s/it]\n",
      "Epochs: 1 | Loss:  0.209 \n",
      "Training is finished!\n",
      "100%|███████████████████████████████████████████| 31/31 [04:04<00:00,  7.88s/it]\n",
      "Epochs: 1 | Loss:  0.265 \n",
      "Training is finished!\n",
      "100%|███████████████████████████████████████████| 31/31 [04:05<00:00,  7.93s/it]\n",
      "Epochs: 1 | Loss:  0.321 \n",
      "Training is finished!\n",
      "100%|███████████████████████████████████████████| 31/31 [04:06<00:00,  7.95s/it]\n",
      "Epochs: 1 | Loss:  0.284 \n",
      "Training is finished!\n",
      "100%|███████████████████████████████████████████| 31/31 [04:07<00:00,  8.00s/it]\n",
      "Epochs: 1 | Loss:  0.380 \n",
      "Training is finished!\n",
      "vit_pairwise_model_emb/model.pth\n",
      "Model is saved: vit_pairwise_model_emb/model.pth\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch \\\n",
    "train_Clip_ViT_fullfc.py --model_name 'openai/clip-vit-large-patch14-336' \\\n",
    "--train_data_file '../data/train/train_vit_pairwise.json' --test_data_file '../data/train/test_vit_pairwise.json' \\\n",
    "--image_dir '../data/images' --max_epoch 1 \\\n",
    "--batch_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7bd0c-dafa-4215-9e9a-a3747af8d5b5",
   "metadata": {},
   "source": [
    "## eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97514bd8-a61f-4076-995b-d90153aa16da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 0 ns, total: 3 μs\n",
      "Wall time: 6.44 μs\n",
      "是否有GPU可用: True\n",
      "GPU数量: 8\n",
      "Namespace(model_name='vit_pairwise_model_emb/model.pth', in_channels=1032, train_data_file=None, test_data_file='../data/train/val_vit.json', image_dir='../data/images', batch_size=8, num_cls=3, max_epoch=1, lr=1e-05, weight_decay=0.005, num_workers=0, print_interval_steps=100, use_score_model=1, use_dropout=0, keep_prob=0.8, clip_value=1.0, save_path='vit_pairwise_model_emb', save_name=None)\n",
      "模型加载用时: 2.3159291744232178\n",
      "!!!!!! 1906\n",
      " 95%|█████████████████████████████████████  | 1814/1906 [13:25<00:44,  2.07it/s]"
     ]
    }
   ],
   "source": [
    "%time\n",
    "!python infer_Clip_ViT_fullfc.py \\\n",
    "--model_name 'vit_pairwise_model_emb/model.pth' \\\n",
    "--test_data_file '../data/train/val_vit.json' \\\n",
    "--image_dir '../data/images' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8beac2-ab5d-47f5-a843-8ce98929ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f58a4-f906-459e-8cfa-b8c69beb227c",
   "metadata": {},
   "source": [
    "## single infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d68d418-6e61-41e6-9e21-9206c7e57290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local_infer_single.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile local_infer_single.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import argparse\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor, AutoTokenizer, AutoModel\n",
    "\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size  # 获得图像宽高\n",
    "    if width == height:  # 相等直接返回不用重搞\n",
    "        return pil_img\n",
    "    elif width > height:  # w大构建w尺寸图\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))  # w最大，以坐标x=0,y=(width - height) // 2位置粘贴原图\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result\n",
    "    \n",
    "class Pairwise_ViT_Infer(torch.nn.Module):\n",
    "    def __init__(self, vision_tower, num_labels=2):\n",
    "        super(Pairwise_ViT_Infer, self).__init__()\n",
    "\n",
    "        self.vit = vision_tower\n",
    "        self.score_layer = torch.nn.Linear(4096, 1)\n",
    "\n",
    "    def forward(self, text1, text2, x1, x2):\n",
    "        x1 = self.vit(x1, output_hidden_states=True)['last_hidden_state']\n",
    "        x2 = self.vit(x2, output_hidden_states=True)['last_hidden_state']\n",
    "\n",
    "        # print(x1[:, 0, :].shape)\n",
    "        # print(text1.shape)\n",
    "        feature1 = torch.concat([x1[:, 0, :], text1.squeeze(dim=1), x2[:, 0, :], text2.squeeze(dim=1)], dim=-1)\n",
    "        # Use the embedding of [CLS] token\n",
    "        output1 = self.score_layer(feature1)\n",
    "\n",
    "        return output1\n",
    "    \n",
    "def init_model(model_path):\n",
    "    ## load first-page-pic-scoring model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_name=os.path.join(model_path)\n",
    "\n",
    "    vit_image_processor = CLIPImageProcessor.from_pretrained('openai/clip-vit-large-patch14-336')  # 加载图像预处理\n",
    "    vision_tower = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14-336')  # 加载图像模型\n",
    "    vit_model = Pairwise_ViT_Infer(vision_tower).to(device)\n",
    "    vit_model.load_state_dict(torch.load(model_name, map_location=device), strict=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5', model_max_length=512)\n",
    "    emb_model = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\n",
    "\n",
    "    vit_image_processor = vit_image_processor\n",
    "    vit_model = vit_model\n",
    "    return tokenizer,emb_model,vit_image_processor,vit_model\n",
    "\n",
    "###\n",
    "def process_image(img_path,vit_image_processor,device):\n",
    "    image1 = Image.open(img_path).convert(\"RGB\")\n",
    "    image1 = expand2square(image1, tuple(int(x * 255) for x in vit_image_processor.image_mean))\n",
    "    image1 = vit_image_processor.preprocess(image1, return_tensors='pt')['pixel_values'][0].unsqueeze(0).to(device)    \n",
    "    return image1\n",
    "\n",
    "def get_inputs(tokenizer,emb_model,text1, text2, device):\n",
    "        # Tokenize sentences\n",
    "        encoded_input1 = tokenizer(text1, padding=True, truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "        text_output1 = torch.nn.functional.normalize(emb_model(**encoded_input1)[0][:, 0], p=2, dim=1).to(device)\n",
    "\n",
    "        encoded_input2 = tokenizer(text2, padding=True, truncation=True,\n",
    "                                        return_tensors='pt')\n",
    "        \n",
    "        text_output2 = torch.nn.functional.normalize(emb_model(**encoded_input2)[0][:, 0], p=2, dim=1).to(device)\n",
    "\n",
    "        return text_output1,text_output2\n",
    "\n",
    "def main(model_path, img1, img2, text1, text2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  #  model_path = 'vit_pairwise_model_emb/model.pth'\n",
    "    tokenizer,emb_model,vit_image_processor,vit_model = init_model(model_path)\n",
    "    \n",
    "    image1 = process_image(img1,vit_image_processor,device)\n",
    "    image2 = process_image(img2,vit_image_processor,device) \n",
    "\n",
    "    text1, text2 = get_inputs(tokenizer,emb_model,text1, text2, device)\n",
    "\n",
    "    score = vit_model(text1, text2, image1, image2)\n",
    "    \n",
    "    print(score)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_path', type=str, default='vit_pairwise_model_emb/model.pth')\n",
    "    parser.add_argument('--image1', type=str)\n",
    "    parser.add_argument('--text1', type=str)\n",
    "    parser.add_argument('--image2', type=str)\n",
    "    parser.add_argument('--text2', type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.model_path, args.image1, args.text1, args.image2, args.text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79f7eeba-2ecc-4196-801a-6a57fac0463f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.5115]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "CPU times: user 37.2 ms, sys: 107 ms, total: 144 ms\n",
      "Wall time: 9.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python local_infer_single.py --model_path 'vit_pairwise_model_emb/model.pth' \\\n",
    "--image1 '../data/images/41H+cMdoqqL._AC_US200_.jpg' \\\n",
    "--text1 '../data/images/61bf5e+XrAL._AC_US200_.jpg'\\\n",
    "--image2 'Gewerbe, Industrie & Wissenschaft:Materialtransport, Ladungssicherung & Zubehör:Zieh- & Hebevorrichtungen:Haken' \\\n",
    "--text2 'Beauty & Personal Care:Hair Care:Hair Extensions, Wigs & Accessories:Wigs'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
